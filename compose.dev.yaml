services:
  ray-head:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ray-head
    # vLLM + Ray are started by start.sh
    ports:
      - "127.0.0.1:8000:8000"   # vLLM
      - "127.0.0.1:8265:8265"   # Ray dashboard
      - "127.0.0.1:8080:8080"   # Ray metrics
      - "127.0.0.1:9090:9090"   # Prometheus
      - "127.0.0.1:3000:3000"   # Grafana
    volumes:
      - ./:/app
      - /models:/models           # <-- persistent HF cache / local models
      - ray-data:/tmp/ray
    environment:
      - RAY_ADDRESS=auto
      - HF_TOKEN=${HF_TOKEN:-}    # if your model is gated; else omit
      - VLLM_MODEL=Qwen/Qwen2.5-1.5B-Instruct
      - VLLM_SERVED_NAME=qwen-1.5b
      - VLLM_DOWNLOAD_DIR=/models
      - VLLM_PORT=8000
      - VLLM_DTYPE=auto
    gpus: all
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      # check both endpoints
      test: ["CMD-SHELL", "curl -fsS http://localhost:8265 && curl -fsS http://localhost:8000/v1/models"]
      interval: 10s
      timeout: 3s
      retries: 20
    networks:
      - ray-network

  # jobs-runner unchanged; just point to vLLM via service DNS (ray-head:8000)
  jobs-runner:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      ray-head:
        condition: service_healthy
    environment:
      - RAY_ADDRESS=ray://ray-head:10001
      - VLLM_URL=http://ray-head:8000/v1/chat/completions
      - VLLM_MODEL=qwen-1.5b
    working_dir: /app
    entrypoint: ["bash", "-lc"]
    command: "python src/batch_infer.py"
    volumes:
      - ./:/app
    networks:
      - ray-network
    gpus: all

networks:
  ray-network:
    driver: bridge

volumes:
  ray-data:
