{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1 Batch Inference with Ray Data LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Ray Data LLM for batch inference with the Meta Llama 3.1 model. This implementation requires significant GPU resources - please ensure your system meets the minimum requirements listed in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Ray - this will connect to the running Ray cluster\n",
    "ray.init(address=\"auto\")\n",
    "print(\"Ray initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure the vLLM Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Llama models, we need to configure the vLLM processor with appropriate parameters to manage memory efficiently. The reference system used for this project has an RTX 2080 with 8GB VRAM, which is at the minimum threshold for running the 8B model. The settings below are optimized for hardware with similar specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "\n",
    "# These parameters are optimized for an RTX 2080 with 8GB VRAM\n",
    "# IMPORTANT: These reduced parameters are essential to fit the model in 8GB VRAM\n",
    "config = vLLMEngineProcessorConfig(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # Using the 8B model as it's most likely to fit\n",
    "    engine_kwargs={\n",
    "        \"enable_chunked_prefill\": True,  # Helps with memory efficiency\n",
    "        \"max_num_batched_tokens\": 2048,  # Reduced from 4096 to save memory\n",
    "        \"max_model_len\": 8192,           # Reduced from 16384 to save memory\n",
    "        \"gpu_memory_utilization\": 0.85,  # Control memory utilization\n",
    "        \"tensor_parallel_size\": 1        # No model parallelism for single GPU\n",
    "    },\n",
    "    concurrency=1,  # Single concurrent worker to avoid memory issues\n",
    "    batch_size=16   # Reduced batch size to avoid OOM errors (64 is default)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the Llama model, you'll need to authenticate with Hugging Face. Make sure you have access to the Llama model on Hugging Face.\n",
    "\n",
    "Important: You need to have requested and been granted access to Meta's Llama models on Hugging Face before proceeding. The model won't download if you don't have proper access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# You need a Hugging Face token with access to Meta Llama models\n",
    "# Get your token at https://huggingface.co/settings/tokens\n",
    "# Set this before running the notebook\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Set in notebook (not recommended for sharing)\n",
    "# os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "# Option 2: Use existing token (preferred)\n",
    "if \"HUGGING_FACE_HUB_TOKEN\" in os.environ:\n",
    "    login(token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "    print(\"Logged in to Hugging Face Hub using environment token\")\n",
    "else:\n",
    "    print(\"WARNING: No Hugging Face token found in environment\")\n",
    "    print(\"Please add your token to environment with:\")\n",
    "    print(\"export HUGGING_FACE_HUB_TOKEN='your_token_here'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the LLM Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build the processor with pre and post-processing functions. This configures how the inputs are formatted for the model and how the outputs are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": row[\"prompt\"] if \"prompt\" in row else row[\"question\"]}\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.7,\n",
    "            max_tokens=256,  # Limited for memory efficiency\n",
    "            top_p=0.9\n",
    "        )\n",
    "    ),\n",
    "    postprocess=lambda row: dict(\n",
    "        response=row[\"generated_text\"],\n",
    "        **row  # This will return all the original columns in the dataset\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset with diverse prompts to test the model. We're using a small number of prompts to avoid memory issues during batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset with a variety of prompts\n",
    "prompts = [\n",
    "    \"Explain the concept of batch inference with LLMs in simple terms.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"What are the main advantages of using Ray for distributed computing?\",\n",
    "    \"Summarize the key features of vLLM in three bullet points.\",\n",
    "    \"How does tensor parallelism improve LLM inference?\"\n",
    "]\n",
    "\n",
    "# Create a Ray dataset\n",
    "ds = ray.data.from_items([{\"prompt\": p} for p in prompts])\n",
    "print(f\"Created dataset with {ds.count()} prompts\")\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll process the dataset through the Llama model. This step may take some time and requires significant GPU resources. We've included error handling in case of memory issues.  \n",
    "\n",
    "Warning: The first run will download the model weights which may take significant time depending on your internet connection. The model is approximately 5-6GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Starting batch inference with Llama 3.1-8B...\")\n",
    "    print(\"This may take a while depending on your hardware.\")\n",
    "    print(\"First run will download model weights (~5-6GB)\")\n",
    "    \n",
    "    # Process the dataset through the Llama model\n",
    "    result_ds = processor(ds)\n",
    "    \n",
    "    print(\"Batch inference completed successfully!\")\n",
    "    result_ds.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch inference: {e}\")\n",
    "    print(\"\\nThis could be due to insufficient GPU memory or other resource constraints.\")\n",
    "    print(\"Consider these troubleshooting steps:\")\n",
    "    print(\"1. Further reduce batch_size (try 8 or 4)\")\n",
    "    print(\"2. Lower max_num_batched_tokens to 1024\")\n",
    "    print(\"3. Reduce gpu_memory_utilization to 0.7\")\n",
    "    print(\"4. Close other applications using GPU memory\")\n",
    "    print(\"5. Check if your WSL GPU passthrough is properly configured\")\n",
    "    print(\"6. Consider trying a smaller model like TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Examine the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch and display the results in a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Take all results and display them\n",
    "    results = result_ds.take_all()\n",
    "    \n",
    "    for i, item in enumerate(results):\n",
    "        print(f\"Prompt {i+1}: {item['prompt']}\")\n",
    "        print(f\"\\nResponse:\\n{item['response']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print()\n",
    "except NameError:\n",
    "    print(\"No results to display. Batch inference may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory and Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check GPU memory usage and other resources. Monitoring memory usage is critical when working with large models on systems with limited VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Only works if you have nvidia-smi available\n",
    "    !nvidia-smi\n",
    "except:\n",
    "    print(\"nvidia-smi not available or not accessible from the notebook\")\n",
    "    \n",
    "    # Alternative memory check using Python\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Cached memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always shut down Ray when you're done to free up resources. This is especially important when working with limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Shutdown Ray to free resources\n",
    "ray.shutdown()\n",
    "print(\"Ray has been shut down.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Recommendations for Limited Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference hardware specification (Intel Core i7-10875H, 32GB RAM, RTX 2080 with 8GB VRAM) presents some specific challenges:\n",
    "\n",
    "1. The RTX 2080 with 8GB VRAM is at the minimum threshold for running Llama-3.1-8B\n",
    "2. WSL adds overhead to GPU performance and memory management\n",
    "3. System processes may compete for GPU resources\n",
    "\n",
    "If encountering persistent memory issues:\n",
    "\n",
    "1. Try using WSL memory configuration to optimize memory allocation\n",
    "2. Create a `.wslconfig` file in the Windows user directory with:\n",
    "   ```\n",
    "   [wsl2]\n",
    "   memory=24GB\n",
    "   swap=8GB\n",
    "   ```\n",
    "3. Consider using a smaller model first to test the workflow\n",
    "4. Run the Ray Dashboard (http://localhost:8265) to monitor cluster resources\n",
    "5. Add `swap_ratio=0.5` to `config` to enable swap memory (slower but helps with OOM)\n",
    "\n",
    "Remember that batch inference is memory-intensive, but the optimized configuration provided in this notebook should work on systems with 8GB VRAM with careful management."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
